nohup: ignoring input
-1.0
06/19/2021 18:38:58 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

loading configuration file ../CatChoice/saved/large_val/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "multi_label_classification",
  "transformers_version": "4.6.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Didn't find file ../CatChoice/saved/large_val/added_tokens.json. We won't load it.
loading file ../CatChoice/saved/large_val/vocab.json
loading file ../CatChoice/saved/large_val/merges.txt
loading file ../CatChoice/saved/large_val/tokenizer.json
loading file None
loading file ../CatChoice/saved/large_val/special_tokens_map.json
loading file ../CatChoice/saved/large_val/tokenizer_config.json
loading weights file ../CatChoice/saved/large_val/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../CatChoice/saved/large_val.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
06/19/2021 18:39:03 - WARNING - datasets.builder -    Using custom data configuration default-9cfe6cc00be1dac5
0 tables [00:00, ? tables/s]1 tables [00:00,  5.13 tables/s]                                Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /dhome/boewoei0123/.cache/huggingface/datasets/json/default-9cfe6cc00be1dac5/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /dhome/boewoei0123/.cache/huggingface/datasets/json/default-9cfe6cc00be1dac5/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
 #0:   0%|          | 0/21 [00:00<?, ?ba/s]
 #1:   0%|          | 0/21 [00:00<?, ?ba/s][A

 #2:   0%|          | 0/21 [00:00<?, ?ba/s][A[A


 #3:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A
 #1:   5%|â–         | 1/21 [00:00<00:16,  1.25ba/s][A #0:   5%|â–         | 1/21 [00:00<00:17,  1.13ba/s]

 #2:   5%|â–         | 1/21 [00:00<00:18,  1.08ba/s][A[A


 #3:   5%|â–         | 1/21 [00:00<00:17,  1.12ba/s][A[A[A #0:  10%|â–‰         | 2/21 [00:01<00:15,  1.22ba/s]


 #3:  10%|â–‰         | 2/21 [00:01<00:15,  1.19ba/s][A[A[A

 #2:  10%|â–‰         | 2/21 [00:01<00:17,  1.11ba/s][A[A
 #1:  10%|â–‰         | 2/21 [00:02<00:19,  1.02s/ba][A #0:  14%|â–ˆâ–        | 3/21 [00:02<00:14,  1.22ba/s]


 #3:  14%|â–ˆâ–        | 3/21 [00:02<00:15,  1.17ba/s][A[A[A

 #2:  14%|â–ˆâ–        | 3/21 [00:02<00:15,  1.13ba/s][A[A #0:  19%|â–ˆâ–‰        | 4/21 [00:03<00:13,  1.29ba/s]
 #1:  14%|â–ˆâ–        | 3/21 [00:03<00:16,  1.07ba/s][A


 #3:  19%|â–ˆâ–‰        | 4/21 [00:03<00:14,  1.20ba/s][A[A[A

 #2:  19%|â–ˆâ–‰        | 4/21 [00:03<00:14,  1.19ba/s][A[A #0:  24%|â–ˆâ–ˆâ–       | 5/21 [00:03<00:11,  1.35ba/s]
 #1:  19%|â–ˆâ–‰        | 4/21 [00:03<00:14,  1.14ba/s][A


 #3:  24%|â–ˆâ–ˆâ–       | 5/21 [00:04<00:12,  1.23ba/s][A[A[A

 #2:  24%|â–ˆâ–ˆâ–       | 5/21 [00:04<00:13,  1.17ba/s][A[A #0:  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:04<00:11,  1.35ba/s]
 #1:  24%|â–ˆâ–ˆâ–       | 5/21 [00:04<00:14,  1.11ba/s][A


 #3:  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:04<00:12,  1.23ba/s][A[A[A

 #2:  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:05<00:12,  1.20ba/s][A[A
 #1:  29%|â–ˆâ–ˆâ–Š       | 6/21 [00:05<00:13,  1.11ba/s][A


 #3:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:05<00:11,  1.25ba/s][A[A[A

 #2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:05<00:11,  1.22ba/s][A[A #0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:05<00:13,  1.06ba/s]
 #1:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:06<00:11,  1.18ba/s][A


 #3:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:06<00:10,  1.20ba/s][A[A[A #0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:06<00:11,  1.10ba/s]

 #2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:06<00:10,  1.19ba/s][A[A
 #1:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:07<00:11,  1.18ba/s][A


 #3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:07<00:09,  1.22ba/s][A[A[A

 #2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:07<00:10,  1.18ba/s][A[A #0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:07<00:11,  1.04ba/s]


 #3:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:08<00:08,  1.24ba/s][A[A[A
 #1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:08<00:10,  1.15ba/s][A

 #2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:08<00:09,  1.18ba/s][A[A #0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:08<00:10,  1.09ba/s]


 #3:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:08<00:07,  1.29ba/s][A[A[A
 #1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:08<00:09,  1.20ba/s][A

 #2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:09<00:08,  1.20ba/s][A[A #0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:09<00:08,  1.12ba/s]


 #3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:09<00:07,  1.25ba/s][A[A[A
 #1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:09<00:09,  1.11ba/s][A

 #2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:10<00:07,  1.20ba/s][A[A #0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:10<00:08,  1.11ba/s]


 #3:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:10<00:06,  1.22ba/s][A[A[A

 #2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:10<00:06,  1.18ba/s][A[A
 #1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:10<00:08,  1.08ba/s][A


 #3:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:11<00:05,  1.22ba/s][A[A[A #0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:11<00:07,  1.01ba/s]
 #1:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:11<00:06,  1.15ba/s][A

 #2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:11<00:05,  1.20ba/s][A[A


 #3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:12<00:04,  1.20ba/s][A[A[A
 #1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:12<00:05,  1.18ba/s][A

 #2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:12<00:04,  1.23ba/s][A[A #0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:12<00:07,  1.02s/ba]


 #3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:13<00:04,  1.19ba/s][A[A[A
 #1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:13<00:05,  1.18ba/s][A

 #2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:13<00:04,  1.21ba/s][A[A #0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:13<00:05,  1.05ba/s]


 #3:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:13<00:03,  1.24ba/s][A[A[A

 #2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:14<00:03,  1.20ba/s][A[A
 #1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:14<00:04,  1.17ba/s][A #0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:14<00:04,  1.08ba/s]


 #3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:14<00:02,  1.24ba/s][A[A[A

 #2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:14<00:02,  1.22ba/s][A[A
 #1:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:15<00:03,  1.17ba/s][A #0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:15<00:03,  1.10ba/s]


 #3:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:15<00:01,  1.24ba/s][A[A[A

 #2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:15<00:01,  1.21ba/s][A[A
 #1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:15<00:02,  1.19ba/s][A #0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:16<00:02,  1.10ba/s]


 #3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:16<00:00,  1.23ba/s][A[A[A


 #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:16<00:00,  1.66ba/s][A[A[A #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:16<00:00,  1.28ba/s]

 #2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:16<00:00,  1.23ba/s][A[A
 #1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:16<00:01,  1.20ba/s][A

 #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:16<00:00,  1.67ba/s][A[A #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:16<00:00,  1.26ba/s] #0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:16<00:01,  1.10ba/s]
 #1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:17<00:00,  1.20ba/s][A
 #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:17<00:00,  1.62ba/s][A #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:17<00:00,  1.19ba/s] #0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:17<00:00,  1.12ba/s] #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:17<00:00,  1.17ba/s]


06/19/2021 18:39:24 - INFO - __main__ -    
******** Running evaluating ********
06/19/2021 18:39:24 - INFO - __main__ -    Num test examples = 84358
06/19/2021 19:02:48 - INFO - pred_utils -    Post-processing example predictions.
06/19/2021 19:03:17 - INFO - pred_utils -    Post-processing example references.
JGA = 0.62378
